{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Duplicate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and results in members answering multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "Follow the steps outlined below to build the appropriate classifier model. \n",
    "\n",
    "\n",
    "Steps:\n",
    "- Download data\n",
    "- Exploration\n",
    "- Cleaning\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "\n",
    "By the end of this project you should have **a presentation that describes the model you built** and its **performance**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gensim\n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "There is no designated test.csv file. The train.csv file is the entire dataset. Part of the data in the train.csv file should be set aside to act as the final testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404287, 6)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# drop na\n",
    "df.dropna(inplace=True)\n",
    "print(df.shape)\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "# define a function for text cleaning\n",
    "def clean_text(text):\n",
    "    # tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # remove number\n",
    "    words = [w for w in words if not w.isdigit()]\n",
    "    # remove stopwords\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # remove punctuations\n",
    "    words = [w.translate(table) for w in words]\n",
    "    # # lemmatize\n",
    "    # words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    # stemming\n",
    "    words = [porter.stem(w) for w in words]\n",
    "    return words\n",
    "\n",
    "# text cleaning\n",
    "df['q1_clean'] = df['question1'].apply(clean_text)\n",
    "df['q2_clean'] = df['question2'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255024\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check target distribution \n",
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    149263\n",
      "0    149263\n",
      "Name: is_duplicate, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_clean</th>\n",
       "      <th>q2_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366758</th>\n",
       "      <td>1</td>\n",
       "      <td>[what, best, horror, movi, ]</td>\n",
       "      <td>[what, best, horror, movi, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240037</th>\n",
       "      <td>0</td>\n",
       "      <td>[whi, , trillion, rich, rothschild, forb, rich...</td>\n",
       "      <td>[who, richest, peopl, trinidad, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382835</th>\n",
       "      <td>1</td>\n",
       "      <td>[how, cold, gobi, desert, get, , averag, tempe...</td>\n",
       "      <td>[how, cold, gobi, desert, get, , averag, tempe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335899</th>\n",
       "      <td>0</td>\n",
       "      <td>[what, differ, follow, sentenc, ]</td>\n",
       "      <td>[what, differ, follow, sentenc, without, , , ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297820</th>\n",
       "      <td>1</td>\n",
       "      <td>[what, reason, behind, abrupt, remov, cyru, mi...</td>\n",
       "      <td>[whi, tata, son, sack, cyru, mistri, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate                                           q1_clean  \\\n",
       "366758             1                       [what, best, horror, movi, ]   \n",
       "240037             0  [whi, , trillion, rich, rothschild, forb, rich...   \n",
       "382835             1  [how, cold, gobi, desert, get, , averag, tempe...   \n",
       "335899             0                  [what, differ, follow, sentenc, ]   \n",
       "297820             1  [what, reason, behind, abrupt, remov, cyru, mi...   \n",
       "\n",
       "                                                 q2_clean  \n",
       "366758                       [what, best, horror, movi, ]  \n",
       "240037                  [who, richest, peopl, trinidad, ]  \n",
       "382835  [how, cold, gobi, desert, get, , averag, tempe...  \n",
       "335899     [what, differ, follow, sentenc, without, , , ]  \n",
       "297820             [whi, tata, son, sack, cyru, mistri, ]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Undersampling, randomly sample class 0 to get same number of rows with class 1\n",
    "# number of rows with target 1\n",
    "n_rows_1 = df.is_duplicate.value_counts()[1]\n",
    "# separate two classes and undersample class 0\n",
    "df_1 = df[df.is_duplicate == 1][['is_duplicate', 'q1_clean', 'q2_clean']]\n",
    "df_0 = df[df.is_duplicate == 0][['is_duplicate', 'q1_clean', 'q2_clean']].sample(n=n_rows_1, random_state=66)\n",
    "# concate two classes and shuffle\n",
    "df_clean = pd.concat([df_1, df_0]).sample(frac=1, random_state=66)\n",
    "print(df_clean.is_duplicate.value_counts())\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since tf-idf or count vectorizers are fed with all questions;\n",
    "# split train/test sets first, then map test set with train-fitted vectorizer to avoid data leakage\n",
    "# also, fit both q1 and q2 in trainset, then separatly transform q1 and q2\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['q1_clean', 'q2_clean']], df['is_duplicate'], test_size=0.25, random_state=66)\n",
    "\n",
    "# concatenate clean text questions in one series for vectorization fitting\n",
    "train_questions = pd.concat([X_train.q1_clean, X_train.q2_clean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, use_idf=True, lowercase=False)\n",
    "# vectorization\n",
    "tfidf.fit(train_questions)\n",
    "# transform questions\n",
    "train_q1 = tfidf.transform(X_train['q1_clean'])\n",
    "train_q2 = tfidf.transform(X_train['q2_clean'])\n",
    "test_q1 = tfidf.transform(X_test['q1_clean'])\n",
    "test_q2 = tfidf.transform(X_test['q2_clean']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(q1, q2):\n",
    "    return q1 + q2\n",
    "def sub(q1, q2):\n",
    "    return np.abs(q1-q2)\n",
    "def multiply(q1, q2):\n",
    "    return q1.multiply(q2)\n",
    "\n",
    "train_add = add(train_q1, train_q2)\n",
    "train_sub = sub(train_q1, train_q2)\n",
    "train_multiply = multiply(train_q1, train_q2)\n",
    "\n",
    "test_add = add(test_q1, test_q2)\n",
    "test_sub = sub(test_q1, test_q2)\n",
    "test_multiply = multiply(test_q1, test_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'\\n-----------{model}-----------')\n",
    "    print('confusion matrix: ', confusion_matrix(y_test, y_pred))\n",
    "    print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('f1 score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Vectors Difference of Pair Questions as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------BernoulliNB()-----------\n",
      "confusion matrix:  [[47375 16462]\n",
      " [10191 27044]]\n",
      "accuracy:  0.7362968972613583\n",
      "f1 score:  0.6698950966671207\n",
      "\n",
      "-----------LogisticRegression()-----------\n",
      "confusion matrix:  [[53285 10552]\n",
      " [12669 24566]]\n",
      "accuracy:  0.7702528890296026\n",
      "f1 score:  0.6790596105206419\n",
      "[19:14:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "-----------XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)-----------\n",
      "confusion matrix:  [[57515  6322]\n",
      " [17685 19550]]\n",
      "accuracy:  0.762476254551211\n",
      "f1 score:  0.6195826136561712\n"
     ]
    }
   ],
   "source": [
    "# input sub of q1 and q2 only\n",
    "predict(BernoulliNB(), train_sub, y_train, test_sub, y_test)\n",
    "predict(LogisticRegression(), train_sub, y_train, test_sub, y_test)\n",
    "predict(XGBClassifier(), train_sub, y_train, test_sub, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------BernoulliNB()-----------\n",
      "confusion matrix:  [[46738 17099]\n",
      " [10310 26925]]\n",
      "accuracy:  0.7288170808928289\n",
      "f1 score:  0.6626958244625211\n",
      "\n",
      "-----------LogisticRegression()-----------\n",
      "confusion matrix:  [[54563  9274]\n",
      " [16123 21112]]\n",
      "accuracy:  0.7487236821275922\n",
      "f1 score:  0.6244214075509088\n",
      "[19:15:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "-----------XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)-----------\n",
      "confusion matrix:  [[56211  7626]\n",
      " [16886 20349]]\n",
      "accuracy:  0.7574798163685293\n",
      "f1 score:  0.6241067320963043\n"
     ]
    }
   ],
   "source": [
    "# input add of q1 and q2 only\n",
    "predict(BernoulliNB(), train_add, y_train, test_add, y_test)\n",
    "predict(LogisticRegression(), train_add, y_train, test_add, y_test)\n",
    "predict(XGBClassifier(), train_add, y_train, test_add, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------BernoulliNB()-----------\n",
      "confusion matrix:  [[56088  7749]\n",
      " [18812 18423]]\n",
      "accuracy:  0.7372071394649359\n",
      "f1 score:  0.581103032788178\n",
      "\n",
      "-----------LogisticRegression()-----------\n",
      "confusion matrix:  [[56726  7111]\n",
      " [17901 19334]]\n",
      "accuracy:  0.7525328478708248\n",
      "f1 score:  0.6072236180904523\n",
      "[19:15:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "-----------XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)-----------\n",
      "confusion matrix:  [[57549  6288]\n",
      " [19739 17496]]\n",
      "accuracy:  0.7424905018204844\n",
      "f1 score:  0.5734607253478425\n"
     ]
    }
   ],
   "source": [
    "# input multiply of q1 and q2 only\n",
    "predict(BernoulliNB(), train_multiply, y_train, test_multiply, y_test)\n",
    "predict(LogisticRegression(), train_multiply, y_train, test_multiply, y_test)\n",
    "predict(XGBClassifier(), train_multiply, y_train, test_multiply, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------BernoulliNB()-----------\n",
      "confusion matrix:  [[44987 18850]\n",
      " [ 8740 28495]]\n",
      "accuracy:  0.7270262782966598\n",
      "f1 score:  0.6737999527074959\n",
      "\n",
      "-----------LogisticRegression()-----------\n",
      "confusion matrix:  [[54976  8861]\n",
      " [10944 26291]]\n",
      "accuracy:  0.8040505778059205\n",
      "f1 score:  0.7264011493776507\n",
      "[19:17:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "-----------XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)-----------\n",
      "confusion matrix:  [[56348  7489]\n",
      " [15763 21472]]\n",
      "accuracy:  0.7699461769827449\n",
      "f1 score:  0.6487401051423047\n"
     ]
    }
   ],
   "source": [
    "# input both add and sub of q1 and q2\n",
    "train = scipy.sparse.hstack([train_add, train_sub])\n",
    "test = scipy.sparse.hstack([test_add, test_sub])\n",
    "\n",
    "predict(BernoulliNB(), train, y_train, test, y_test)\n",
    "predict(LogisticRegression(), train, y_train, test, y_test)\n",
    "predict(XGBClassifier(), train, y_train, test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------BernoulliNB()-----------\n",
      "confusion matrix:  [[50148 13689]\n",
      " [10598 26637]]\n",
      "accuracy:  0.7597059521924965\n",
      "f1 score:  0.6868658217403077\n",
      "\n",
      "-----------LogisticRegression()-----------\n",
      "confusion matrix:  [[55305  8532]\n",
      " [11728 25507]]\n",
      "accuracy:  0.7995488364730093\n",
      "f1 score:  0.7157448719027976\n",
      "[19:18:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "-----------XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)-----------\n",
      "confusion matrix:  [[56535  7302]\n",
      " [15926 21309]]\n",
      "accuracy:  0.7701836314706348\n",
      "f1 score:  0.6472374935455456\n"
     ]
    }
   ],
   "source": [
    "# input both sub and multiply of q1 and q2\n",
    "train = scipy.sparse.hstack([train_sub, train_multiply])\n",
    "test = scipy.sparse.hstack([test_sub, test_multiply])\n",
    "\n",
    "predict(BernoulliNB(), train, y_train, test, y_test)\n",
    "predict(LogisticRegression(), train, y_train, test, y_test)\n",
    "predict(XGBClassifier(), train, y_train, test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------BernoulliNB()-----------\n",
      "confusion matrix:  [[50058 13779]\n",
      " [11209 26026]]\n",
      "accuracy:  0.7527703023587146\n",
      "f1 score:  0.6756490134994808\n",
      "\n",
      "-----------LogisticRegression()-----------\n",
      "confusion matrix:  [[55274  8563]\n",
      " [13952 23283]]\n",
      "accuracy:  0.7772380085483616\n",
      "f1 score:  0.6740782559603943\n",
      "[19:19:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "-----------XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)-----------\n",
      "confusion matrix:  [[56314  7523]\n",
      " [16758 20477]]\n",
      "accuracy:  0.7597653158144689\n",
      "f1 score:  0.6277918295393576\n"
     ]
    }
   ],
   "source": [
    "# input both add and multiply of q1 and q2\n",
    "train = scipy.sparse.hstack([train_add, train_multiply])\n",
    "test = scipy.sparse.hstack([test_add, test_multiply])\n",
    "\n",
    "predict(BernoulliNB(), train, y_train, test, y_test)\n",
    "predict(LogisticRegression(), train, y_train, test, y_test)\n",
    "predict(XGBClassifier(), train, y_train, test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------BernoulliNB()-----------\n",
      "confusion matrix:  [[47148 16689]\n",
      " [ 9169 28066]]\n",
      "accuracy:  0.7441625771727086\n",
      "f1 score:  0.684620075618978\n",
      "\n",
      "-----------LogisticRegression()-----------\n",
      "confusion matrix:  [[54952  8885]\n",
      " [10936 26299]]\n",
      "accuracy:  0.803892274813994\n",
      "f1 score:  0.7263011088250321\n",
      "[19:20:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "-----------XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)-----------\n",
      "confusion matrix:  [[56520  7317]\n",
      " [15727 21508]]\n",
      "accuracy:  0.77200411587779\n",
      "f1 score:  0.6511656070239177\n"
     ]
    }
   ],
   "source": [
    "# input both add and multiply of q1 and q2\n",
    "train = scipy.sparse.hstack([train_add, train_sub, train_multiply])\n",
    "test = scipy.sparse.hstack([test_add, test_sub, test_multiply])\n",
    "\n",
    "predict(BernoulliNB(), train, y_train, test, y_test)\n",
    "predict(LogisticRegression(), train, y_train, test, y_test)\n",
    "predict(XGBClassifier(), train, y_train, test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[41377 22460]\n",
      " [ 6672 30563]]\n",
      "accuracy:  0.7117698274497388\n",
      "recall:  0.8208137505035584\n",
      "precision:  0.5764102370669332\n"
     ]
    }
   ],
   "source": [
    "# input sub of q1 and q2 only\n",
    "predict(BernoulliNB(), bow_train_sub, y_train, bow_test_sub, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[46738 17099]\n",
      " [10310 26925]]\n",
      "accuracy:  0.7288170808928289\n",
      "recall:  0.7231099771720155\n",
      "precision:  0.6115982191531891\n"
     ]
    }
   ],
   "source": [
    "# input add of q1 and q2 only\n",
    "predict(BernoulliNB(), bow_train_add, y_train, bow_test_add, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[56088  7749]\n",
      " [18812 18423]]\n",
      "accuracy:  0.7372071394649359\n",
      "recall:  0.4947764200349134\n",
      "precision:  0.703920220082531\n"
     ]
    }
   ],
   "source": [
    "# input multiply of q1 and q2 only\n",
    "predict(BernoulliNB(), bow_train_multiply, y_train, bow_test_multiply, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[42744 21093]\n",
      " [ 6399 30836]]\n",
      "accuracy:  0.72799588412221\n",
      "recall:  0.8281455619712635\n",
      "precision:  0.593810780103603\n"
     ]
    }
   ],
   "source": [
    "# input both add and sub of q1 and q2\n",
    "bow_train = scipy.sparse.hstack([bow_train_add, bow_train_sub])\n",
    "bow_test = scipy.sparse.hstack([bow_test_add, bow_test_sub])\n",
    "\n",
    "predict(BernoulliNB(), bow_train, y_train, bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[50058 13779]\n",
      " [11209 26026]]\n",
      "accuracy:  0.7527703023587146\n",
      "recall:  0.6989660265878878\n",
      "precision:  0.6538374576058285\n"
     ]
    }
   ],
   "source": [
    "# input both add and multiply of q1 and q2\n",
    "bow_train = scipy.sparse.hstack([bow_train_add, bow_train_multiply])\n",
    "bow_test = scipy.sparse.hstack([bow_test_add, bow_test_multiply])\n",
    "\n",
    "predict(BernoulliNB(), bow_train, y_train, bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------Naive Bayes-----------\n",
      "confusion matrix:  [[46029 17808]\n",
      " [ 6698 30537]]\n",
      "accuracy:  0.7575391799905018\n",
      "recall:  0.8201154827447295\n",
      "precision:  0.631647533354018\n",
      "\n",
      "------------Logistic-------------\n",
      "confusion matrix:  [[53889  9948]\n",
      " [11096 26139]]\n",
      "accuracy:  0.7917919898686085\n",
      "recall:  0.7020008056935679\n",
      "precision:  0.7243328622495635\n",
      "\n",
      "-------------xgboost-------------\n",
      "[19:50:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "confusion matrix:  [[57306  6531]\n",
      " [18604 18631]]\n",
      "accuracy:  0.7513158936203894\n",
      "recall:  0.5003625621055459\n",
      "precision:  0.7404419362530801\n"
     ]
    }
   ],
   "source": [
    "# input both sub and multiply of q1 and q2\n",
    "bow_train = scipy.sparse.hstack([bow_train_sub, bow_train_multiply])\n",
    "bow_test = scipy.sparse.hstack([bow_test_sub, bow_test_multiply])\n",
    "\n",
    "print('\\n-----------Naive Bayes-----------')\n",
    "predict(BernoulliNB(), bow_train, y_train, bow_test, y_test)\n",
    "print('\\n------------Logistic-------------')\n",
    "predict(LogisticRegression(), bow_train, y_train, bow_test, y_test)\n",
    "print('\\n-------------xgboost-------------')\n",
    "predict(XGBClassifier(), bow_train, y_train, bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[45114 18723]\n",
      " [ 6913 30322]]\n",
      "accuracy:  0.7463590311856894\n",
      "recall:  0.8143413455082583\n",
      "precision:  0.6182485472525232\n"
     ]
    }
   ],
   "source": [
    "# input add, sub and multiply of q1 and q2\n",
    "bow_train = scipy.sparse.hstack([bow_train_add, bow_train_sub, bow_train_multiply])\n",
    "bow_test = scipy.sparse.hstack([bow_test_add, bow_test_sub, bow_test_multiply])\n",
    "\n",
    "predict(BernoulliNB(), bow_train, y_train, bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[47333 16504]\n",
      " [ 8442 28793]]\n",
      "accuracy:  0.7531858477125217\n",
      "recall:  0.7732778299986571\n",
      "precision:  0.6356491599885202\n"
     ]
    }
   ],
   "source": [
    "# input q1, q2, sub and multiply of q1 and q2\n",
    "bow_train = scipy.sparse.hstack([bow_train_q1, bow_train_q2, bow_train_sub, bow_train_multiply])\n",
    "bow_test = scipy.sparse.hstack([bow_test_q1, bow_test_q2, bow_test_sub, bow_test_multiply])\n",
    "\n",
    "predict(BernoulliNB(), bow_train, y_train, bow_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:  [[44987 18850]\n",
      " [ 8740 28495]]\n",
      "accuracy:  0.7270262782966598\n",
      "recall:  0.7652746072243857\n",
      "precision:  0.6018586968000845\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "t_c_train = scipy.sparse.hstack([tfidf_train, bow_train])\n",
    "t_c_test = scipy.sparse.hstack([tfidf_test, bow_test])\n",
    "\n",
    "predict(BernoulliNB(), t_c_train, y_train, t_c_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------Naive Bayes-----------\n",
      "confusion matrix:  [[47246 16591]\n",
      " [ 7616 29619]]\n",
      "accuracy:  0.7604974671521292\n",
      "recall:  0.7954612595676112\n",
      "precision:  0.6409651590564813\n",
      "\n",
      "------------Logistic-------------\n",
      "confusion matrix:  [[53728 10109]\n",
      " [10712 26523]]\n",
      "accuracy:  0.7939983378185848\n",
      "recall:  0.7123136833624278\n",
      "precision:  0.7240390915046954\n",
      "\n",
      "-------------xgboost-------------\n",
      "[19:51:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "confusion matrix:  [[56070  7767]\n",
      " [15145 22090]]\n",
      "accuracy:  0.7733101155611841\n",
      "recall:  0.5932590304820733\n",
      "precision:  0.7398599993301403\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "t_c_train = scipy.sparse.hstack([tfidf_train_sub, tfidf_train_multiply, bow_train_sub, bow_train_multiply])\n",
    "t_c_test = scipy.sparse.hstack([tfidf_test_sub, tfidf_test_multiply, bow_test_sub, bow_test_multiply])\n",
    "\n",
    "print('\\n-----------Naive Bayes-----------')\n",
    "predict(BernoulliNB(), t_c_train, y_train, t_c_test, y_test)\n",
    "print('\\n------------Logistic-------------')\n",
    "predict(LogisticRegression(), t_c_train, y_train, t_c_test, y_test)\n",
    "print('\\n-------------xgboost-------------')\n",
    "predict(XGBClassifier(), t_c_train, y_train, t_c_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
